# Micro-Tiny GPT-2 Model FROM SCRATCH
This is our micro-tiny GPT-2 model (üòÅ we are still learning), built from scratch and inspired by the innovative approaches of Hugging Face Transformers and OpenAI architectures. Developed during our internship at [3D Smart Factory](https://3dsmartfactory.csit.ma/), this model showcases our committed efforts to create an advanced AI solution.


<div align="center">
  <h5>Micro-Tiny-GPT</h5>
  <img src="https://github-production-user-asset-6210df.s3.amazonaws.com/89405673/264447768-39a2a1a9-ca80-44cd-9d83-fd1358cb379b.png" width="200" height="200" />
</div>

## NOTE

**Important:** This project involves fine-tuning various GPT-2 family models (small, medium, large, etc.) to develop two distinct chatbots: one for question and answer interactions [here](https://github.com/benisalla/fine-tune-gpt-2-chatbot) and another for context-based question and answer interactions [here](https://github.com/benisalla/fine-tune-gpt2_on_context_Q-A). Additionally, we have implemented a question and answer system based on the core transformer architecture which was initially used for translation tasks [here](https://github.com/benisalla/chatbot-based-translation-transformer).


**Note:** This is a learning project and does not represent a production-grade solution. The focus is on educational growth and experimentation.



<aside class="notice" style="background-color:#FFFFE0; border:2px solid #E0E000; padding:10px;">
**Note:**
This is an important notice. It provides additional information or a reminder.
</aside>





## Table of Contents

- [Introduction](#introduction)
- [Features](#features)
- [Getting Started](#getting-started)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
- [Usage](#usage)
- [Model Architecture](#model-architecture)
- [Contributing](#contributing)
- [Acknowledgements](#acknowledgements)
- [License](#license)

## Introduction

Welcome to the Micro-Tiny GPT-2 Model repository! This project is an exploration into building a compact GPT-2 model from scratch, taking inspiration from the Hugging Face Transformers and OpenAI architectures. Developed during an internship at 3D Smart Factory, this model represents our dedication to creating advanced AI solutions despite our learning phase.







## Background on GPT-2

The Generative Pre-trained Transformer 2 (GPT-2), developed by OpenAI, is the second installment in their fundamental series of GPT models. GPT-2 was pretrained on the BookCorpus dataset, consisting of over 7,000 unpublished fiction books of various genres, and then fine-tuned on a dataset comprising 8 million web pages. It was partially unveiled in February 2019, followed by the full release of the 1.5 billion parameter model on November 5, 2019.

![GPT-2 Model](images/gpt-2.jpg)

GPT-2 represents a "direct scale-up" from its predecessor, GPT-1, with a tenfold increase in both the number of parameters and the size of the training dataset. This versatile model owes its ability to perform various tasks to its intrinsic capacity to accurately predict the next element in a sequence. This predictive capability enables GPT-2 to accomplish tasks such as text translation, answering questions based on textual content, summarizing text passages, and generating text that can sometimes closely resemble human style. However, it may exhibit repetitive or nonsensical behavior when generating long passages.

There is a family of GPT-2 models; below, we can see the pretrained GPT-2 model family:

![GPT-2 Model Family](images/gpt-2-family.jpg)










## Features

- **Micro-Tiny Scale:** This GPT-2 model is intentionally designed to be on a micro-tiny scale, showcasing our learning journey and commitment to innovation.

- **Inspired by Industry Leaders:** Influenced by the Hugging Face Transformers and OpenAI architectures, we've incorporated industry best practices into our model's design.

- **Internship Project:** Developed during our internship at 3D Smart Factory, this project reflects real-world experience and hands-on learning.

## Getting Started

### Prerequisites

- Python 3.x
- TensorFlow [Version]
- NumPy [Version]

## Installation

1. Clone the repository:

   ```sh
   git clone https://github.com/yourusername/micro-tiny-gpt2.git
   cd micro-tiny-gpt2
    pip install -r requirements.txt
   ```


### Usage

## Usage

To use the Micro-Tiny GPT-2 model, follow these steps:

1. [Additional usage instructions or code snippets if necessary]

2. [Example usage scenarios or code snippets]

## Model Architecture

The Micro-Tiny GPT-2 model is built with simplicity and learning in mind. While it may not match the scale of larger models, it represents our foundational steps towards understanding complex AI architectures.

[Include a brief explanation of your model's architecture, design choices, and any notable components.]

## Contributing

We welcome contributions from the community to enhance this project. To contribute, please follow these steps:

1. Fork the repository.
2. Create a new branch.
3. Make your enhancements.
4. Test thoroughly.
5. Submit a pull request.

## Acknowledgements

We would like to express our gratitude to [Mentor's Name] at 3D Smart Factory for guiding us throughout the development of this project.

## Acknowledgements

We would like to express our gratitude to [Mentor's Name] at 3D Smart Factory for guiding us throughout the development of this project.




## Inference

Upon the conclusion of the fine-tuning process, the model is poised to engage in inference, crafting answers from a contextual backdrop. Provide a succinct yet comprehensive guide on effectively leveraging the trained model for insightful question answering.







## Contributors

Acknowledge the significant contributions of your colleagues, attributing specific roles and responsibilities to each individual:

- [Ben Alla Ismail (me üòÉ)](https://github.com/benisalla)
- [Agrat Mohammed](https://github.com/agrat)
- [Mhaoui Siham](https://github.com/siham)
- [X Souhayle](https://github.com/yourusername)
- [X Daoudi](https://github.com/yourusername)





## Contact Us
For inquiries or suggestions, please contact:
- Project Lead: Ben alla ismail ([ismailbenalla52@gmail.com](mailto:ismailbenalla52@gmail.com))
- Co-lead: mhaoui Siham ([mahouisiham@gmail.com](mailto:mahouisiham@gmail.com))




## About Me

üéì I'm Ismail Ben Alla, and I have a deep passion for neural networks üòç. My mission is to assist neural networks in unraveling the mysteries of our universe.</br>
‚õµ I'm an enthusiast when it comes to AI, Deep Learning, and Machine Learning algorithms.</br>
‚úÖ I'm an optimist and a dedicated hard worker, constantly striving to push the boundaries of what's possible.</br>
üå± I'm committed to continuously learning and staying updated with advanced computer science technologies.</br>
üòÑ I absolutely love what I do, and I'm excited about the endless possibilities in the world of AI and machine learning!</br>

Let's connect and explore the fascinating world of artificial intelligence together! ü§ñüåü


<div align="center">
  <a href="https://twitter.com/ismail_ben_alla" target="blank">
    <img src="https://raw.githubusercontent.com/rahuldkjain/github-profile-readme-generator/master/src/images/icons/Social/twitter.svg" alt="ismail_ben_alla" height="30" width="40" />
  </a>
  <a href="https://linkedin.com/in/ismail-ben-alla-7144b5221/" target="blank">
    <img src="https://raw.githubusercontent.com/rahuldkjain/github-profile-readme-generator/master/src/images/icons/Social/linked-in-alt.svg" alt="ismail-ben-alla-7144b5221/" height="30" width="40" />
  </a>
  <a href="https://instagram.com/ismail_ben_alla" target="blank">
    <img src="https://raw.githubusercontent.com/rahuldkjain/github-profile-readme-generator/master/src/images/icons/Social/instagram.svg" alt="ismail_ben_alla" height="30" width="40" />
  </a>
</div>






<div align="center">
  <h4>You are about to witness some pure magic ‚ú®üé© !! Ta-da!</h4>
  <img src="https://github-production-user-asset-6210df.s3.amazonaws.com/89405673/270190847-0b3ee23b-c082-483e-9e12-8b15a1b8f0a3.gif" width="500" height="300"/>
</div>




























# OverView

This chapter provides an in-depth exploration of the meticulous process involved in constructing the foundational GPT-2 model from its inception. Our journey encompasses crucial phases, including data collection, model architecture design, training protocols, and practical applications. Throughout this chapter, we shed light on the intricate nature of developing such a powerful language model and the profound potential it holds for various applications in the field of natural language processing.

Welcome to the Micro-Tiny GPT-2 Model repository! This project is an exploration into building a compact GPT-2 model from scratch, taking inspiration from the Hugging Face Transformers and OpenAI architectures. Developed during an internship at 3D Smart Factory, this model represents our dedication to creating advanced AI solutions despite our learning phase.











## Data Preparation

### WebText (OpenWebTextCorpus)

To train GPT-2, OpenAI needed a substantial corpus of 40 GB of high-quality text. While Common Crawl provided the necessary scale for modern language models, its quality was often inconsistent. Manually curating data from Common Crawl was an option but a costly one. Fortunately, Reddit provided a decentralized curation approach by design, proving to be a crucial innovation for creating the WebText dataset.

The WebText generation process can be summarized as follows:

1. Retrieve URLs of all Reddit submissions until December 2017 with a score of 3 or higher.
2. Deduplicate retrieved content based on URLs.
3. Exclude Wikipedia content, as OpenAI already had a separate Wikipedia dataset.
4. Further deduplicate the remaining content using an undisclosed "heuristic" cleaning method, including the removal of non-English web pages.

Neither the resulting corpus nor the source code for its generation was made public, which later inspired Aaron Gokaslan and Vanya Cohen to create the OpenWebText corpus.

### OpenWebText

OpenWebText2 is an enhanced version of the original OpenWebText corpus, covering all Reddit submissions from 2005 to April 2020, with additional months becoming available after the publication of corresponding PushShift backup files.

![OpenWebText](images/openwebtext.png)

Due to resource constraints, it is important to note that we trained GPT-2 on only a quarter of the OpenWebText dataset. This limitation in training data was necessary to optimize computational resources while still achieving significant language model performance.

### Data Preprocessing

We combined approximately 5 million files from the OpenWebText dataset, roughly equivalent to a quarter of the entire OpenWebText corpus. Subsequently, we performed the following steps:

1. We used the GPT-2 tokenizer, also known as "r50k," to tokenize the dataset.
2. Following established conventions for dataset splitting, we divided the data into training and validation sets, allocating 80% for training and 10% for validation.
3. To optimize data management and efficiency, we stored the data as a binary stream in the 'train.bin' and 'val.bin' files.

Our dataset is currently accessible on Kaggle:

![OpenWebText by Ben Alla Ismail](images/ben-alla-openwebtext.jpeg)










## Model Architecture

The architecture of GPT-2, a groundbreaking language model, represents a notable evolution of deep learning-based transformer models. Initially, it followed the traditional transformer architecture with both encoder and decoder components, but subsequent research simplified the design by removing one.

![GPT-2 Relationship with Transformers](images/gpt-relationship-with-bert-transformers.jpg)

This led to models with exceptionally high stacks of transformer blocks and massive volumes of training data, often requiring significant computational resources and costs. This chapter explores the architecture of GPT-2 and its relationship with transformers, highlighting innovative developments that shaped its evolution into a powerful language model.

![Full GPT-2 Architecture](images/Full_GPT_architecture.png)
