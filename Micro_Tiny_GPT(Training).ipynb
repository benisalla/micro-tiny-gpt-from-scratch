{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWsFqyZ9eX-p"
      },
      "source": [
        "#Importing required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UZFoU3R_uq1"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUlpvHPV3yLI"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "import time\n",
        "import pickle\n",
        "from contextlib import nullcontext\n",
        "import numpy as np\n",
        "import time\n",
        "import tiktoken\n",
        "import pandas as pd\n",
        "import json\n",
        "import tarfile\n",
        "import lzma"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WQBgWM0E_8N"
      },
      "source": [
        "#The Core of GPT-2 ðŸ¤–"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oucOCqDjE_8X"
      },
      "source": [
        "#####Layer Norm (Normalization Layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFpx13ezE_8Y"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAc1C3leE_8Y"
      },
      "source": [
        "#####Causal Self Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_j1wuZwE_8Y"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0 ; \"n_embd % n_head should be 0.\"\n",
        "\n",
        "        self.mh_atten_ln = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "\n",
        "        self.proj_ln = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "\n",
        "        self.atten_drop = nn.Dropout(config.drop_rate)\n",
        "        self.res_drop = nn.Dropout(config.drop_rate)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.drop_rate = config.drop_rate\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: Works only on PyTorch version 2.0 or higher.\")\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v  = self.mh_atten_ln(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        if self.flash:\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.drop_rate if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.atten_drop(att)\n",
        "            y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        y = self.res_drop(self.proj_ln(y))\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N55PUpvIE_8Y"
      },
      "source": [
        "#####MLP(Multi-Layer Perceptron)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JzLud8YPE_8Z"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.fc_ln    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.proj_ln  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.drop_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.gelu(self.fc_ln(x))\n",
        "        return self.dropout(self.proj_ln(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sa0PxWlHE_8Z"
      },
      "source": [
        "#####Block(communication + computation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6-UHl49E_8Z"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn_net = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.ff_mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn_net(self.ln_1(x))\n",
        "        return x + self.ff_mlp(self.ln_2(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHKtywaQE_8a"
      },
      "source": [
        "#####Body of GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWBZQyGFE_8a"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None, \"vocab_size is required !!\"\n",
        "        assert config.block_size is not None, \"block_size is required !!\"\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            tok_embedding = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            pos_embedding = nn.Embedding(config.block_size, config.n_embd),\n",
        "            dropout = nn.Dropout(config.drop_rate),\n",
        "            MHS_Attn_Block = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.tok_embedding.weight = self.lm_head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        for param_name, param in self.named_parameters():\n",
        "            if param_name.endswith('proj_ln.weight'):\n",
        "                torch.nn.init.normal_(param, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.pos_embedding.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        B, T = idx.size()\n",
        "\n",
        "        assert T <= self.config.block_size, f\"Max sequence length is {self.config.block_size}\"\n",
        "\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=device)\n",
        "        tok_emb = self.transformer.tok_embedding(idx)\n",
        "        pos_emb = self.transformer.pos_embedding(pos)\n",
        "        x = self.transformer.dropout(tok_emb + pos_emb)\n",
        "        for block in self.transformer.MHS_Attn_Block:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            logits = self.lm_head(x[:, [-1], :])\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def crop_block_size(self, block_size):\n",
        "        assert block_size <= self.config.block_size, \"new block_size should be < old block_size\"\n",
        "        self.config.block_size = block_size\n",
        "\n",
        "        self.transformer.pos_embedding.weight = nn.Parameter(self.transformer.pos_embedding.weight[:block_size])\n",
        "\n",
        "        for block in self.transformer.MHS_Attn_Block:\n",
        "            if hasattr(block.attn, 'bias'):\n",
        "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ek0m7bDvOYA"
      },
      "source": [
        "#Initializing Model and Train it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2W6KSDaEvBP"
      },
      "source": [
        "#####Model Configuration and Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDZ1eN2qE0H8",
        "outputId": "3eaee38d-4fb6-49e7-8ac7-76644fe1ff16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'data_path': '/content/drive/MyDrive/GPT_FROM_SCRATCH/Done/file_00/file_00.txt', 'idx': 999, 'src_path': '/content/drive/MyDrive/GPT_FROM_SCRATCH/subsets/urlsf_subset00.tar', 'dist_path': '/content/drive/MyDrive/GPT_FROM_SCRATCH/Done/file_00', 'in_dir': '/content/drive/MyDrive/GPT_FROM_SCRATCH/Done/file_00/openwebtext'}\n"
          ]
        }
      ],
      "source": [
        "checkpts_dir = '/content/drive/MyDrive/GPT_FROM_SCRATCH/check_points'\n",
        "eval_interval = 2000\n",
        "log_interval = 1\n",
        "eval_iters = 200\n",
        "save_checkpoint = True\n",
        "init_from = \"scratch\"\n",
        "\n",
        "# data\n",
        "db_name = 'openwebtext'\n",
        "num_steps = 200\n",
        "batch_size = 12\n",
        "block_size = 1024\n",
        "\n",
        "# model\n",
        "n_layer = 12\n",
        "n_head = 12\n",
        "n_embd = 768\n",
        "dropout = 0.0\n",
        "bias = False\n",
        "\n",
        "# adamw optimizer\n",
        "learning_rate = 6e-4\n",
        "max_iters = 600000\n",
        "weight_decay = 1e-1\n",
        "beta1 = 0.9\n",
        "beta2 = 0.95\n",
        "grad_clip = 1.0\n",
        "\n",
        "# Learning rate decay\n",
        "decay_lr = True\n",
        "warmup_iters = 2000\n",
        "lr_decay_iters = 600000\n",
        "min_lr = 6e-5\n",
        "\n",
        "# system\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\n",
        "compile = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXRq4gNoFf1B"
      },
      "source": [
        "#####Data Loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VE_ClAdVFh-2"
      },
      "outputs": [],
      "source": [
        "data_dir = os.path.join('/content/drive/MyDrive/GPT_FROM_SCRATCH', db_name)\n",
        "\n",
        "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
        "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
        "\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
        "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
        "    if device == 'cuda':\n",
        "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
        "    else:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-aCol3iE1KJ"
      },
      "source": [
        "#####Initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTWlLfCGxcJi"
      },
      "source": [
        "> GPT-2 Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZD29VGy11LXz"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    dropout: float = 0.0\n",
        "    bias: bool = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2qkNNUvjvNrQ",
        "outputId": "c29bc197-1407-46ec-d706-df505d705c6b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing The Model from : resume\n",
            "number of parameters: 123.59M\n",
            "scaler : <torch.cuda.amp.grad_scaler.GradScaler object at 0x7d7847f67d60>\n",
            "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
            "num non-decayed parameter tensors: 25, with 19,200 parameters\n",
            "using fused AdamW: True\n",
            "Compiling the model... (Estimated duration: ~= 1min)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(2000)\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "\n",
        "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
        "ctx = nullcontext() if device == 'cpu' else torch.amp.autocast(device_type=device, dtype=ptdtype)\n",
        "\n",
        "\n",
        "# Note: init_from='resume' or ...)\n",
        "init_from='resume'\n",
        "iter_num = 0\n",
        "best_val_loss = 1e9\n",
        "\n",
        "# Model\n",
        "model_args = dict(n_layer=n_layer,\n",
        "                  n_head=n_head,\n",
        "                  n_embd=n_embd,\n",
        "                  block_size=block_size,\n",
        "                  bias=bias,\n",
        "                  vocab_size=None,\n",
        "                  drop_rate=dropout)\n",
        "\n",
        "\n",
        "# Building Model\n",
        "print(f\"Initializing The Model from : {init_from}\")\n",
        "\n",
        "if init_from == 'scratch':\n",
        "    model_args['vocab_size'] =  50304\n",
        "    gptconf = Config(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "\n",
        "elif init_from == 'resume':\n",
        "    print(\"Loading checkpoints ...\")\n",
        "    checkpoint = None\n",
        "    if os.path.exists(save_ckpt_path):\n",
        "        checkpoint = torch.load(save_ckpt_path, map_location=device)\n",
        "        print(\"==> loading from new checkpoints \")\n",
        "    else:\n",
        "        checkpoint = torch.load(load_ckpt_path, map_location=device)\n",
        "        print(\"==> loading from old checkpoints \")\n",
        "\n",
        "    checkpoint_model_args = checkpoint['model_args']\n",
        "    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
        "        model_args[k] = checkpoint_model_args[k]\n",
        "    gptconf = Config(**model_args)\n",
        "    model = GPT(gptconf)\n",
        "    state_dict = checkpoint['model']\n",
        "\n",
        "    unwanted_prefix = '_orig_mod.' # this prefix was added while saving check points\n",
        "    for key, value in list(state_dict.items()):\n",
        "        if key.startswith(unwanted_prefix):\n",
        "            state_dict[key[len(unwanted_prefix):]] = state_dict.pop(key)\n",
        "    model.load_state_dict(state_dict)\n",
        "    iter_num = checkpoint['iter_num']\n",
        "    best_val_loss = checkpoint['best_val_loss']\n",
        "    print(\"checkpoints loaded successfully :)\")\n",
        "else:\n",
        "    print(\"Oups, There is no such option !!\")\n",
        "\n",
        "\n",
        "# Crop block size of the original model\n",
        "if block_size < model.config.block_size:\n",
        "    model.crop_block_size(block_size)\n",
        "    model_args['block_size'] = block_size\n",
        "model.to(device)\n",
        "\n",
        "# initialize a GradScaler\n",
        "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
        "\n",
        "print(f\"scaler : {scaler}\")\n",
        "\n",
        "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device)\n",
        "if init_from == 'resume':\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "checkpoint = None\n",
        "\n",
        "if compile:\n",
        "    print(\"Compiling the model... \")\n",
        "    model = torch.compile(model) # requires PyTorch 2.0\n",
        "    print(\"Model compiled successfully :)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtrbXkYB9q1i"
      },
      "source": [
        "#####Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZRCWIMf9ua5"
      },
      "outputs": [],
      "source": [
        "# Estimation of Loss\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split, block_size, batch_size)\n",
        "            with ctx:\n",
        "                logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "\n",
        "# Learning Rate Update\n",
        "def get_lr(it):\n",
        "    w = warmup_iters\n",
        "    lr = learning_rate\n",
        "    lr_di = lr_decay_iters\n",
        "    if it < w:\n",
        "        return lr * it / w\n",
        "    if it > lr_di:\n",
        "        return min_lr\n",
        "    decay_ratio = (it - w) / (lr_di - w)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
        "    return min_lr + coeff * (lr - min_lr)\n",
        "\n",
        "\n",
        "# Save Checkpoints\n",
        "def save_checkpts(model, optimizer, ckpt_path, model_args, iter_num, best_val_loss):\n",
        "      print(\"Saving checkpoints ...\")\n",
        "      checkpoint = {\n",
        "          'model': raw_model.state_dict(),\n",
        "          'optimizer': optimizer.state_dict(),\n",
        "          'model_args': model_args,\n",
        "          'iter_num': iter_num,\n",
        "          'best_val_loss': best_val_loss,\n",
        "          }\n",
        "      time.sleep(2)\n",
        "      print(\"Checkpoints Saved Successfully :)\")\n",
        "      torch.save(checkpoint, ckpt_path)\n",
        "\n",
        "\n",
        "# Evaluate Model\n",
        "def evaluate_model(iter_num, model, optimizer,model_args, eval_interval, best_val_loss, save_ckpt_path, save_checkpoint):\n",
        "    if iter_num % eval_interval == 0:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "        if losses['val'] < best_val_loss or save_checkpoint:\n",
        "            best_val_loss = losses['val']\n",
        "            if iter_num > 0:\n",
        "                save_checkpts(\n",
        "                    model=model,\n",
        "                    optimizer=optimizer,\n",
        "                    ckpt_path=save_ckpt_path,\n",
        "                    model_args=model_args,\n",
        "                    iter_num=iter_num,\n",
        "                    best_val_loss=best_val_loss,\n",
        "                )\n",
        "\n",
        "    return iter_num, best_val_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MwQv0C-hhewh"
      },
      "source": [
        "#####Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxRewAPnhiqR"
      },
      "outputs": [],
      "source": [
        "X, Y = get_batch('train', block_size, batch_size)\n",
        "t0 = time.time()\n",
        "local_iter_num = 0\n",
        "raw_model =  model\n",
        "running_mfu = -1.0\n",
        "\n",
        "while True:\n",
        "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "\n",
        "    # evaluation (Train / val)\n",
        "    iter_num, best_val_loss = evaluate_model(iter_num,\n",
        "                                             raw_model,\n",
        "                                             optimizer,\n",
        "                                             model_args,\n",
        "                                             eval_interval,\n",
        "                                             best_val_loss,\n",
        "                                             save_ckpt_path,\n",
        "                                             save_checkpoint)\n",
        "\n",
        "    # forward -> backward -> optimizing\n",
        "    for micro_step in range(num_steps):\n",
        "        with ctx:\n",
        "            _ , loss = raw_model(X, Y)\n",
        "            loss = loss / num_steps\n",
        "\n",
        "        X, Y = get_batch('train', block_size, batch_size)\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "    # clip the gradient\n",
        "    if grad_clip != 0.0:\n",
        "        scaler.unscale_(optimizer)\n",
        "        torch.nn.utils.clip_grad_norm_(raw_model.parameters(), grad_clip)\n",
        "\n",
        "    # step the optimizer and scaler if training in fp16\n",
        "    scaler.step(optimizer)\n",
        "    scaler.update()\n",
        "\n",
        "    # free gradients memory\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "    # timing and logging\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0\n",
        "    t0 = t1\n",
        "    if iter_num % log_interval == 0:\n",
        "        lossf = loss.item() * num_steps\n",
        "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n",
        "    iter_num += 1\n",
        "    local_iter_num += 1\n",
        "\n",
        "    # save checkpoints\n",
        "    if iter_num % 10 == 0:\n",
        "        save_checkpts(raw_model, optimizer,save_ckpt_path, model_args, iter_num, best_val_loss)\n",
        "\n",
        "    # exit\n",
        "    if iter_num > 3000 : # > max_iters:\n",
        "        break"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KWsFqyZ9eX-p",
        "6j6kuTp_37wP",
        "fFDaVvLztog3",
        "5Ek0m7bDvOYA",
        "YtrbXkYB9q1i",
        "MwQv0C-hhewh"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
