{"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<center><h1 style=\"text-align: center; font-size: 36px; color: #294; font-weight: bold;\">Micro-Tiny-GPT from scratch</h1></center>\n","metadata":{"id":"gSVAU60m1tit"}},{"cell_type":"markdown","source":"#Importing required libraries.","metadata":{"id":"KWsFqyZ9eX-p"}},{"cell_type":"code","source":"%%capture\n!pip install tiktoken","metadata":{"id":"6UZFoU3R_uq1","execution":{"iopub.status.busy":"2023-09-12T13:37:05.876611Z","iopub.execute_input":"2023-09-12T13:37:05.876889Z","iopub.status.idle":"2023-09-12T13:37:18.381504Z","shell.execute_reply.started":"2023-09-12T13:37:05.876863Z","shell.execute_reply":"2023-09-12T13:37:18.380284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport inspect\nfrom dataclasses import dataclass\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport os\nimport time\nfrom contextlib import nullcontext\nimport numpy as np\nimport time\nimport tiktoken","metadata":{"id":"BUlpvHPV3yLI","execution":{"iopub.status.busy":"2023-09-12T13:37:18.385489Z","iopub.execute_input":"2023-09-12T13:37:18.385850Z","iopub.status.idle":"2023-09-12T13:37:21.895421Z","shell.execute_reply.started":"2023-09-12T13:37:18.385819Z","shell.execute_reply":"2023-09-12T13:37:21.894493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#The Core of GPT-2 ðŸ¤–","metadata":{"id":"6j6kuTp_37wP"}},{"cell_type":"markdown","source":"<center>\n  <img src=\"https://github-production-user-asset-6210df.s3.amazonaws.com/89405673/258571749-4954bee7-47e4-4848-a842-affc25c3cc17.png\" width=\"900\" height=\"260\"/>\n</center>","metadata":{"id":"ANEtXwEcbuXv"}},{"cell_type":"markdown","source":"#####Layer Norm (Normalization Layer)","metadata":{"id":"aqmKyo9gewGn"}},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n\n    \"\"\"\n        Layer Normalization module. (different than batch normalization and better in seq problems)\n\n        Tensor          Type            Shape\n        ===========================================================================\n        input           long            (batch_size, seq_len, n_embd)\n        ---------------------------------------------------------------------------\n        output          float           (batch_size, seq_len, n_embd)\n        ===========================================================================\n    \"\"\"\n\n    def __init__(self, ndim, bias):\n        super().__init__()\n        self.weight = nn.Parameter(torch.ones(ndim))\n        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n\n    def forward(self, input):\n        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)","metadata":{"id":"LUoCCGNi36n5","execution":{"iopub.status.busy":"2023-09-12T13:37:21.898973Z","iopub.execute_input":"2023-09-12T13:37:21.899371Z","iopub.status.idle":"2023-09-12T13:37:21.906690Z","shell.execute_reply.started":"2023-09-12T13:37:21.899343Z","shell.execute_reply":"2023-09-12T13:37:21.905418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#####Causal Self Attention","metadata":{"id":"OlqdCZome_NA"}},{"cell_type":"code","source":"class CausalSelfAttention(nn.Module):\n\n    \"\"\"\n        Causal Self-Attention Module\n        ( why causal? well because attention mechanism is a causal operation not a correlation )\n\n        Tensor          Type            Shape\n        ===========================================================================\n        input           long            (batch_size, seq_len, n_embd)\n        ---------------------------------------------------------------------------\n        output          float           (batch_size, seq_len, n_embd)\n        ===========================================================================\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.n_embd % config.n_head == 0 ; \"n_embd % n_head should be 0.\"\n\n        self.mh_atten_ln = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n\n        self.proj_ln = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n\n        self.atten_drop = nn.Dropout(config.drop_rate)\n        self.res_drop = nn.Dropout(config.drop_rate)\n        self.n_head = config.n_head\n        self.n_embd = config.n_embd\n        self.drop_rate = config.drop_rate\n        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n        if not self.flash:\n            print(\"WARNING: Works only on PyTorch version 2.0 or higher.\")\n            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n                                        .view(1, 1, config.block_size, config.block_size))\n\n    def forward(self, x):\n        B, T, C = x.size()\n        q, k, v  = self.mh_atten_ln(x).split(self.n_embd, dim=2)\n        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n\n        if self.flash:\n            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.drop_rate if self.training else 0, is_causal=True)\n        else:\n            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n            att = F.softmax(att, dim=-1)\n            att = self.atten_drop(att)\n            y = att @ v\n        y = y.transpose(1, 2).contiguous().view(B, T, C)\n\n        y = self.res_drop(self.proj_ln(y))\n        return y\n","metadata":{"id":"Kx8PP9kIWi4a","execution":{"iopub.status.busy":"2023-09-12T13:37:21.908417Z","iopub.execute_input":"2023-09-12T13:37:21.909122Z","iopub.status.idle":"2023-09-12T13:37:21.924445Z","shell.execute_reply.started":"2023-09-12T13:37:21.909072Z","shell.execute_reply":"2023-09-12T13:37:21.923521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#####MLP(Multi-Layer Perceptron)","metadata":{"id":"okOcnTEbGH7d"}},{"cell_type":"code","source":"class MLP(nn.Module):\n\n    \"\"\"\n        Multi-Layer Perceptron (MLP): computation layer.\n        Non-linearity = complex mappings + feature extraction  + ... + processing data (reasoning)\n\n        Tensor          Type            Shape\n        ===========================================================================\n        input           long            (batch_size, seq_len, n_embd)\n        ---------------------------------------------------------------------------\n        output          float           (batch_size, seq_len, n_embd)\n        ===========================================================================\n\n    \"\"\"\n    def __init__(self, config):\n        super().__init__()\n        self.fc_ln    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n        self.gelu    = nn.GELU()\n        self.proj_ln  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n        self.dropout = nn.Dropout(config.drop_rate)\n\n    def forward(self, x):\n        x = self.gelu(self.fc_ln(x))\n        return self.dropout(self.proj_ln(x))","metadata":{"id":"1zYryqXgMDB_","execution":{"iopub.status.busy":"2023-09-12T13:37:21.929126Z","iopub.execute_input":"2023-09-12T13:37:21.929446Z","iopub.status.idle":"2023-09-12T13:37:21.939405Z","shell.execute_reply.started":"2023-09-12T13:37:21.929415Z","shell.execute_reply":"2023-09-12T13:37:21.938480Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#####Block(communication + computation)","metadata":{"id":"Hwr9wcWJq59d"}},{"cell_type":"code","source":"class Block(nn.Module):\n\n    \"\"\"\n        Transformer Block: communication layer (att mechanism) + computation layer ( feedforward).\n\n        Tensor          Type            Shape\n        ===========================================================================\n        input           long            (batch_size, seq_len, n_embd)\n        ---------------------------------------------------------------------------\n        output          float           (batch_size, seq_len, n_embd)\n        ===========================================================================\n    \"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n        self.attn_net = CausalSelfAttention(config)\n        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n        self.ff_mlp = MLP(config)\n\n    def forward(self, x):\n        x = x + self.attn_net(self.ln_1(x))\n        return x + self.ff_mlp(self.ln_2(x))","metadata":{"id":"nibGUXluMGkf","execution":{"iopub.status.busy":"2023-09-12T13:37:21.942678Z","iopub.execute_input":"2023-09-12T13:37:21.942937Z","iopub.status.idle":"2023-09-12T13:37:21.954882Z","shell.execute_reply.started":"2023-09-12T13:37:21.942914Z","shell.execute_reply":"2023-09-12T13:37:21.953949Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#####Config Object","metadata":{"id":"IDjDoa-rrFzV"}},{"cell_type":"code","source":"@dataclass\nclass Config:\n    block_size: int = 1024\n    vocab_size: int = 50304\n    n_layer: int = 12\n    n_head: int = 12\n    n_embd: int = 768\n    drop_rate: float = 0.0\n    bias: bool = True","metadata":{"id":"ZD29VGy11LXz","execution":{"iopub.status.busy":"2023-09-12T13:37:21.956248Z","iopub.execute_input":"2023-09-12T13:37:21.956638Z","iopub.status.idle":"2023-09-12T13:37:21.966405Z","shell.execute_reply.started":"2023-09-12T13:37:21.956607Z","shell.execute_reply":"2023-09-12T13:37:21.965380Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#####Body of GPT","metadata":{"id":"fFDaVvLztog3"}},{"cell_type":"code","source":"class GPT(nn.Module):\n\n    def __init__(self, config):\n        super().__init__()\n        assert config.vocab_size is not None, \"vocab_size is required !!\"\n        assert config.block_size is not None, \"block_size is required !!\"\n\n        self.config = config\n\n        self.transformer = nn.ModuleDict(dict(\n            tok_embedding = nn.Embedding(config.vocab_size, config.n_embd),\n            pos_embedding = nn.Embedding(config.block_size, config.n_embd),\n            dropout = nn.Dropout(config.drop_rate),\n            MHS_Attn_Block = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n        ))\n\n        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n        self.transformer.tok_embedding.weight = self.lm_head.weight\n        self.apply(self._init_weights)\n\n        for param_name, param in self.named_parameters():\n            if param_name.endswith('proj_ln.weight'):\n                torch.nn.init.normal_(param, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n\n        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n\n    def get_num_params(self, non_embedding=True):\n        n_params = sum(p.numel() for p in self.parameters())\n        if non_embedding:\n            n_params -= self.transformer.pos_embedding.weight.numel()\n        return n_params\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx, targets=None):\n        device = idx.device\n        B, T = idx.size()\n\n        assert T <= self.config.block_size, f\"Max sequence length is {self.config.block_size}\"\n\n        pos = torch.arange(0, T, dtype=torch.long, device=device)\n        tok_emb = self.transformer.tok_embedding(idx)\n        pos_emb = self.transformer.pos_embedding(pos)\n        x = self.transformer.dropout(tok_emb + pos_emb)\n        for block in self.transformer.MHS_Attn_Block:\n            x = block(x)\n        x = self.transformer.ln_f(x)\n\n        if targets is not None:\n            logits = self.lm_head(x)\n            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n        else:\n            logits = self.lm_head(x[:, [-1], :])\n            loss = None\n\n        return logits, loss\n\n    def crop_block_size(self, block_size):\n        assert block_size <= self.config.block_size, \"new block_size should be < old block_size\"\n        self.config.block_size = block_size\n\n        self.transformer.pos_embedding.weight = nn.Parameter(self.transformer.pos_embedding.weight[:block_size])\n\n        for block in self.transformer.MHS_Attn_Block:\n            if hasattr(block.attn, 'bias'):\n                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n\n    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n        param_dict = {pn: p for pn, p in self.named_parameters()}\n        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n\n        optim_groups = [\n            {'params': decay_params, 'weight_decay': weight_decay},\n            {'params': nodecay_params, 'weight_decay': 0.0}\n        ]\n\n        num_decay_params = sum(p.numel() for p in decay_params)\n        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n\n        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n\n        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n        use_fused = fused_available and device_type == 'cuda'\n        extra_args = dict(fused=True) if use_fused else dict()\n        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n        print(f\"using fused AdamW: {use_fused}\")\n\n        return optimizer\n\n    @torch.no_grad()\n    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n        for _ in range(max_new_tokens):\n            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n            logits, _ = self(idx_cond)\n            logits = logits[:, -1, :] / temperature\n            if top_k is not None:\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                logits[logits < v[:, [-1]]] = -float('Inf')\n            probs = F.softmax(logits, dim=-1)\n            idx_next = torch.multinomial(probs, num_samples=1)\n            idx = torch.cat((idx, idx_next), dim=1)\n\n        return idx","metadata":{"id":"9QA8gNAZ1ftY","execution":{"iopub.status.busy":"2023-09-12T13:37:21.967895Z","iopub.execute_input":"2023-09-12T13:37:21.968241Z","iopub.status.idle":"2023-09-12T13:37:21.996896Z","shell.execute_reply.started":"2023-09-12T13:37:21.968210Z","shell.execute_reply":"2023-09-12T13:37:21.996014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#Initializing Model and Train it","metadata":{"id":"5Ek0m7bDvOYA"}},{"cell_type":"markdown","source":"#####Model Configuration and Hyperparameters","metadata":{"id":"q2W6KSDaEvBP"}},{"cell_type":"code","source":"data_dir = \"/kaggle/input/gpt-data/GPTData/\"\nsave_ckpt_path = '/kaggle/working/ckpt.pt'\nload_ckpt_path = '/kaggle/input/gpt-ckpts/ckpt.pt'\neval_interval = 20\nlog_interval = 1\neval_iters = 200\nsave_checkpoint = True\ninit_from = \"resume\" # can take resume or scratch as values\n\n# data\ndb_name = 'gpt_data'\nnum_steps = 200\nbatch_size = 16\nblock_size = 512\n\n# model\nn_layer = 12\nn_head = 12\nn_embd = 768\ndropout = 0.0\nbias = False\n\n# adamw optimizer\nlearning_rate = 6e-4\nmax_iters = 600000\nweight_decay = 1e-1\nbeta1 = 0.9\nbeta2 = 0.95\ngrad_clip = 1.0\n\n# Learning rate decay\ndecay_lr = True\nwarmup_iters = 2000\nlr_decay_iters = 600000\nmin_lr = 6e-5\n\n# system\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\ndtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16'\ncompile = False","metadata":{"id":"aDZ1eN2qE0H8","outputId":"3eaee38d-4fb6-49e7-8ac7-76644fe1ff16","execution":{"iopub.status.busy":"2023-09-12T13:37:21.998136Z","iopub.execute_input":"2023-09-12T13:37:21.998489Z","iopub.status.idle":"2023-09-12T13:37:22.047856Z","shell.execute_reply.started":"2023-09-12T13:37:21.998456Z","shell.execute_reply":"2023-09-12T13:37:22.046935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#####Data Loader\n","metadata":{"id":"yXRq4gNoFf1B"}},{"cell_type":"markdown","source":"<center>\n  <h6>Loading Data By chuncks using NumPy Memory Mapped Array</h6>\n  <img src=\"https://github-production-user-asset-6210df.s3.amazonaws.com/89405673/259172272-6434381c-b51a-4974-ba66-399ed7fc5778.png\" />\n</center>","metadata":{"id":"s2ur8ZlEb1jH"}},{"cell_type":"code","source":"train_data = np.memmap(os.path.join(data_dir, 'Train.bin'), dtype=np.uint16, mode='r')\nval_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n\n\ndef get_batch(split, block_size, batch_size):\n    data = train_data if split == 'train' else val_data\n    ix = torch.randint(len(data) - block_size, (batch_size,))\n    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n    if device == 'cuda':\n        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n    else:\n        x, y = x.to(device), y.to(device)\n    return x, y","metadata":{"id":"VE_ClAdVFh-2","execution":{"iopub.status.busy":"2023-09-12T13:37:22.049181Z","iopub.execute_input":"2023-09-12T13:37:22.049523Z","iopub.status.idle":"2023-09-12T13:37:22.067495Z","shell.execute_reply.started":"2023-09-12T13:37:22.049491Z","shell.execute_reply":"2023-09-12T13:37:22.066549Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#####Initialization","metadata":{"id":"W-aCol3iE1KJ"}},{"cell_type":"markdown","source":"<center>\n  <h6>AUTOMATIC MIXED PRECISION</h6>\n  <img src=\"https://github-production-user-asset-6210df.s3.amazonaws.com/89405673/259163442-802ef1b4-8130-44f5-86be-90e34dcd437e.png\" width=\"900\" height=\"200\"/>\n  <img src=\"https://github-production-user-asset-6210df.s3.amazonaws.com/89405673/259163449-0fd88fd1-3c32-4b79-807e-e4e281819a13.png\" width=\"500\" height=\"140\"/>\n</center>","metadata":{"id":"73qoNUPXb6vi"}},{"cell_type":"code","source":"torch.manual_seed(2000)\ntorch.backends.cuda.matmul.allow_tf32 = True\ntorch.backends.cudnn.allow_tf32 = True\n\nptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\nctx = nullcontext() if device == 'cpu' else torch.amp.autocast(device_type=device, dtype=ptdtype)\n\n\n# Note: init_from='resume' or ...)\ninit_from='resume'\niter_num = 0\nbest_val_loss = 1e9\n\n# Model\nmodel_args = dict(n_layer=n_layer,\n                  n_head=n_head,\n                  n_embd=n_embd,\n                  block_size=block_size,\n                  bias=bias,\n                  vocab_size=None,\n                  drop_rate=dropout)\n\n\n# Building Model\nprint(f\"Initializing The Model from : {init_from}\")\n\nif init_from == 'scratch':\n    model_args['vocab_size'] =  50304\n    gptconf = Config(**model_args)\n    model = GPT(gptconf)\n\nelif init_from == 'resume':\n    print(\"Loading checkpoints ...\")\n    checkpoint = None\n    if os.path.exists(save_ckpt_path):\n        checkpoint = torch.load(save_ckpt_path, map_location=device)\n        print(\"==> loading from new checkpoints \")\n    else:\n        checkpoint = torch.load(load_ckpt_path, map_location=device)\n        print(\"==> loading from old checkpoints \")\n        \n    checkpoint_model_args = checkpoint['model_args']\n    for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n        model_args[k] = checkpoint_model_args[k]\n    gptconf = Config(**model_args)\n    model = GPT(gptconf)\n    state_dict = checkpoint['model']\n\n    unwanted_prefix = '_orig_mod.' # this prefix was added while saving check points\n    for key, value in list(state_dict.items()):\n        if key.startswith(unwanted_prefix):\n            state_dict[key[len(unwanted_prefix):]] = state_dict.pop(key)\n    model.load_state_dict(state_dict)\n    iter_num = checkpoint['iter_num']\n    best_val_loss = checkpoint['best_val_loss']\n    print(\"checkpoints loaded successfully :)\")\nelse:\n    print(\"Oups, There is no such option !!\")\n\n\n# Crop block size of the original model\nif block_size < model.config.block_size:\n    model.crop_block_size(block_size)\n    model_args['block_size'] = block_size\nmodel.to(device)\n\n# initialize a GradScaler\nscaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n\nprint(f\"scaler : {scaler}\")\n\noptimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device)\nif init_from == 'resume':\n    optimizer.load_state_dict(checkpoint['optimizer'])\ncheckpoint = None\n\nif compile:\n    print(\"Compiling the model... \")\n    model = torch.compile(model) # requires PyTorch 2.0\n    print(\"Model compiled successfully :)\")","metadata":{"id":"2qkNNUvjvNrQ","outputId":"c29bc197-1407-46ec-d706-df505d705c6b","execution":{"iopub.status.busy":"2023-09-12T13:37:22.069044Z","iopub.execute_input":"2023-09-12T13:37:22.069565Z","iopub.status.idle":"2023-09-12T13:37:40.927123Z","shell.execute_reply.started":"2023-09-12T13:37:22.069532Z","shell.execute_reply":"2023-09-12T13:37:40.925320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#####Utils","metadata":{"id":"YtrbXkYB9q1i"}},{"cell_type":"markdown","source":"<center>\n  <h6><strong>Learning Rate Graph</strong> ( Linear -> Cosine Decay -> lr_min )</h6>\n  <img src=\"https://github-production-user-asset-6210df.s3.amazonaws.com/89405673/259077879-acb08446-83da-46ea-86ea-d4d4f3e332ac.png\" />\n</center>","metadata":{"id":"4amMJIzDcERp"}},{"cell_type":"code","source":"# Estimation of Loss\n@torch.no_grad()\ndef estimate_loss():\n    out = {}\n    model.eval()\n    for split in ['train', 'val']:\n        losses = torch.zeros(eval_iters)\n        for k in range(eval_iters):\n            X, Y = get_batch(split, block_size, batch_size)\n            with ctx:\n                logits, loss = model(X, Y)\n            losses[k] = loss.item()\n        out[split] = losses.mean()\n    model.train()\n    return out\n\n\n# Learning Rate Update\ndef get_lr(it):\n    w = warmup_iters\n    lr = learning_rate\n    lr_di = lr_decay_iters\n    if it < w:\n        return lr * it / w\n    if it > lr_di:\n        return min_lr\n    decay_ratio = (it - w) / (lr_di - w)\n    assert 0 <= decay_ratio <= 1\n    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n    return min_lr + coeff * (lr - min_lr)\n\n\n# Save Checkpoints\ndef save_checkpts(model, optimizer, ckpt_path, model_args, iter_num, best_val_loss):\n      print(\"Saving checkpoints ...\")\n      checkpoint = {\n          'model': raw_model.state_dict(),\n          'optimizer': optimizer.state_dict(),\n          'model_args': model_args,\n          'iter_num': iter_num,\n          'best_val_loss': best_val_loss,\n          }\n      time.sleep(2)\n      print(\"Checkpoints Saved Successfully :)\")\n      torch.save(checkpoint, ckpt_path)\n\n\n# Evaluate Model\ndef evaluate_model(iter_num, model, optimizer,model_args, eval_interval, best_val_loss, save_ckpt_path, save_checkpoint):\n    if iter_num % eval_interval == 0:\n        losses = estimate_loss()\n        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n        if losses['val'] < best_val_loss or save_checkpoint:\n            best_val_loss = losses['val']\n            if iter_num > 0:\n                save_checkpts(\n                    model=model,\n                    optimizer=optimizer,\n                    ckpt_path=save_ckpt_path,\n                    model_args=model_args,\n                    iter_num=iter_num,\n                    best_val_loss=best_val_loss,\n                )\n\n    return iter_num, best_val_loss","metadata":{"id":"dZRCWIMf9ua5","execution":{"iopub.status.busy":"2023-09-12T13:37:40.928601Z","iopub.execute_input":"2023-09-12T13:37:40.928973Z","iopub.status.idle":"2023-09-12T13:37:40.943366Z","shell.execute_reply.started":"2023-09-12T13:37:40.928939Z","shell.execute_reply":"2023-09-12T13:37:40.942297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#####Training Loop","metadata":{"id":"MwQv0C-hhewh"}},{"cell_type":"code","source":"X, Y = get_batch('train', block_size, batch_size)\nt0 = time.time()\nlocal_iter_num = 0\nraw_model =  model\nrunning_mfu = -1.0\n\nwhile True:\n    lr = get_lr(iter_num) if decay_lr else learning_rate\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n\n    # evaluation (Train / val)\n    iter_num, best_val_loss = evaluate_model(iter_num, \n                                             raw_model, \n                                             optimizer,\n                                             model_args,\n                                             eval_interval, \n                                             best_val_loss, \n                                             save_ckpt_path, \n                                             save_checkpoint)\n\n    # forward -> backward -> optimizing\n    for micro_step in range(num_steps):\n        with ctx:\n            _ , loss = raw_model(X, Y)\n            loss = loss / num_steps\n\n        X, Y = get_batch('train', block_size, batch_size)\n        scaler.scale(loss).backward()\n\n    # clip the gradient\n    if grad_clip != 0.0:\n        scaler.unscale_(optimizer)\n        torch.nn.utils.clip_grad_norm_(raw_model.parameters(), grad_clip)\n\n    # step the optimizer and scaler if training in fp16\n    scaler.step(optimizer)\n    scaler.update()\n\n    # free gradients memory\n    optimizer.zero_grad(set_to_none=True)\n\n    # timing and logging\n    t1 = time.time()\n    dt = t1 - t0\n    t0 = t1\n    if iter_num % log_interval == 0:\n        lossf = loss.item() * num_steps\n        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms\")\n    iter_num += 1\n    local_iter_num += 1\n\n    # save checkpoints\n    if iter_num % 10 == 0:\n        save_checkpts(raw_model, optimizer,save_ckpt_path, model_args, iter_num, best_val_loss)\n\n    # exit\n    if iter_num > 3000 : # > max_iters:\n        break","metadata":{"id":"IxRewAPnhiqR","execution":{"iopub.status.busy":"2023-09-12T13:37:40.944974Z","iopub.execute_input":"2023-09-12T13:37:40.945572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<center>\n  <h1>the rest of the code (inference, fine-tuning ...) is on other notebooks ðŸ™‚</h1>\n</center>","metadata":{"id":"l4gHzrE91PRM"}}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}