# Micro-Tiny GPT-2 Model FROM SCRATCH
This is our micro-tiny GPT-2 model (üòÅ we are still learning), built from scratch and inspired by the innovative approaches of Hugging Face Transformers and OpenAI architectures. Developed during our internship at [3D Smart Factory](https://3dsmartfactory.csit.ma/), this model showcases our committed efforts to create an advanced AI solution.


<div align="center">
  <img src="https://github-production-user-asset-6210df.s3.amazonaws.com/89405673/263525109-7872c85c-d387-463f-9477-62704f47ee10.png" width="200" height="200" />
</div>


## NOTE

**Important:** This project involves fine-tuning various GPT-2 family models (small, medium, large, etc.) to develop two distinct chatbots: one for question and answer interactions [here](https://github.com/benisalla/fine-tune-gpt-2-chatbot) and another for context-based question and answer interactions [here](https://github.com/benisalla/fine-tune-gpt2_on_context_Q-A). Additionally, we have implemented a question and answer system based on the core transformer architecture which was initially used for translation tasks [here](https://github.com/benisalla/chatbot-based-translation-transformer).

## Table of Contents

- [Introduction](#introduction)
- [Features](#features)
- [Getting Started](#getting-started)
  - [Prerequisites](#prerequisites)
  - [Installation](#installation)
- [Usage](#usage)
- [Model Architecture](#model-architecture)
- [Contributing](#contributing)
- [Acknowledgements](#acknowledgements)
- [License](#license)

## Introduction

Welcome to the Micro-Tiny GPT-2 Model repository! This project is an exploration into building a compact GPT-2 model from scratch, taking inspiration from the Hugging Face Transformers and OpenAI architectures. Developed during an internship at 3D Smart Factory, this model represents our dedication to creating advanced AI solutions despite our learning phase.

## Features

- **Micro-Tiny Scale:** This GPT-2 model is intentionally designed to be on a micro-tiny scale, showcasing our learning journey and commitment to innovation.

- **Inspired by Industry Leaders:** Influenced by the Hugging Face Transformers and OpenAI architectures, we've incorporated industry best practices into our model's design.

- **Internship Project:** Developed during our internship at 3D Smart Factory, this project reflects real-world experience and hands-on learning.

## Getting Started

### Prerequisites

- Python 3.x
- TensorFlow [Version]
- NumPy [Version]

## Installation

1. Clone the repository:

   ```sh
   git clone https://github.com/yourusername/micro-tiny-gpt2.git
   cd micro-tiny-gpt2
    pip install -r requirements.txt
   ```


### Usage

## Usage

To use the Micro-Tiny GPT-2 model, follow these steps:

1. [Additional usage instructions or code snippets if necessary]

2. [Example usage scenarios or code snippets]

## Model Architecture

The Micro-Tiny GPT-2 model is built with simplicity and learning in mind. While it may not match the scale of larger models, it represents our foundational steps towards understanding complex AI architectures.

[Include a brief explanation of your model's architecture, design choices, and any notable components.]

## Contributing

We welcome contributions from the community to enhance this project. To contribute, please follow these steps:

1. Fork the repository.
2. Create a new branch.
3. Make your enhancements.
4. Test thoroughly.
5. Submit a pull request.

## Acknowledgements

We would like to express our gratitude to [Mentor's Name] at 3D Smart Factory for guiding us throughout the development of this project.

## Acknowledgements

We would like to express our gratitude to [Mentor's Name] at 3D Smart Factory for guiding us throughout the development of this project.


## License

This project is licensed under the [License Name] License - see the LICENSE file for details.

---

**Note:** This is a learning project and does not represent a production-grade solution. The focus is on educational growth and experimentation.

