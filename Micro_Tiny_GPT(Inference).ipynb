{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KWsFqyZ9eX-p"
      },
      "source": [
        "# Importing required libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6UZFoU3R_uq1"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install tiktoken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUlpvHPV3yLI"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import os\n",
        "import time\n",
        "from contextlib import nullcontext\n",
        "import numpy as np\n",
        "import time\n",
        "import tiktoken\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j6kuTp_37wP"
      },
      "source": [
        "# The Core of GPT-2 🤖"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqmKyo9gewGn"
      },
      "source": [
        "##### Layer Norm (Normalization Layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUoCCGNi36n5"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "\n",
        "    def __init__(self, ndim, bias):\n",
        "        super().__init__()\n",
        "        self.weight = nn.Parameter(torch.ones(ndim))\n",
        "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
        "\n",
        "    def forward(self, input):\n",
        "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OlqdCZome_NA"
      },
      "source": [
        "##### Causal Self Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx8PP9kIWi4a"
      },
      "outputs": [],
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0 ; \"n_embd % n_head should be 0.\"\n",
        "\n",
        "        self.mh_atten_ln = nn.Linear(config.n_embd, 3 * config.n_embd, bias=config.bias)\n",
        "\n",
        "        self.proj_ln = nn.Linear(config.n_embd, config.n_embd, bias=config.bias)\n",
        "\n",
        "        self.atten_drop = nn.Dropout(config.drop_rate)\n",
        "        self.res_drop = nn.Dropout(config.drop_rate)\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "        self.drop_rate = config.drop_rate\n",
        "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
        "        if not self.flash:\n",
        "            print(\"WARNING: Works only on PyTorch version 2.0 or higher.\")\n",
        "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
        "                                        .view(1, 1, config.block_size, config.block_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size()\n",
        "        q, k, v  = self.mh_atten_ln(x).split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2)\n",
        "\n",
        "        if self.flash:\n",
        "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.drop_rate if self.training else 0, is_causal=True)\n",
        "        else:\n",
        "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
        "            att = F.softmax(att, dim=-1)\n",
        "            att = self.atten_drop(att)\n",
        "            y = att @ v\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
        "\n",
        "        y = self.res_drop(self.proj_ln(y))\n",
        "        return y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okOcnTEbGH7d"
      },
      "source": [
        "##### MLP(Multi-Layer Perceptron)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1zYryqXgMDB_"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.fc_ln    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.proj_ln  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
        "        self.dropout = nn.Dropout(config.drop_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.gelu(self.fc_ln(x))\n",
        "        return self.dropout(self.proj_ln(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hwr9wcWJq59d"
      },
      "source": [
        "##### Block(communication + computation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nibGUXluMGkf"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.attn_net = CausalSelfAttention(config)\n",
        "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
        "        self.ff_mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn_net(self.ln_1(x))\n",
        "        return x + self.ff_mlp(self.ln_2(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFDaVvLztog3"
      },
      "source": [
        "##### Body of GPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QA8gNAZ1ftY"
      },
      "outputs": [],
      "source": [
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.vocab_size is not None, \"vocab_size is required !!\"\n",
        "        assert config.block_size is not None, \"block_size is required !!\"\n",
        "\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            tok_embedding = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            pos_embedding = nn.Embedding(config.block_size, config.n_embd),\n",
        "            dropout = nn.Dropout(config.drop_rate),\n",
        "            MHS_Attn_Block = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
        "        ))\n",
        "\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "        self.transformer.tok_embedding.weight = self.lm_head.weight\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        for param_name, param in self.named_parameters():\n",
        "            if param_name.endswith('proj_ln.weight'):\n",
        "                torch.nn.init.normal_(param, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
        "\n",
        "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
        "\n",
        "    def get_num_params(self, non_embedding=True):\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        if non_embedding:\n",
        "            n_params -= self.transformer.pos_embedding.weight.numel()\n",
        "        return n_params\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        B, T = idx.size()\n",
        "\n",
        "        assert T <= self.config.block_size, f\"Max sequence length is {self.config.block_size}\"\n",
        "\n",
        "        pos = torch.arange(0, T, dtype=torch.long, device=device)\n",
        "        tok_emb = self.transformer.tok_embedding(idx)\n",
        "        pos_emb = self.transformer.pos_embedding(pos)\n",
        "        x = self.transformer.dropout(tok_emb + pos_emb)\n",
        "        for block in self.transformer.MHS_Attn_Block:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            logits = self.lm_head(x)\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "        else:\n",
        "            logits = self.lm_head(x[:, [-1], :])\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, betas, device_type):\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "\n",
        "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
        "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
        "\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == 'cuda'\n",
        "        extra_args = dict(fused=True) if use_fused else dict()\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=betas, **extra_args)\n",
        "        print(f\"using fused AdamW: {use_fused}\")\n",
        "\n",
        "        return optimizer\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
        "            logits, _ = self(idx_cond)\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ek0m7bDvOYA"
      },
      "source": [
        "# Inference"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTWlLfCGxcJi"
      },
      "source": [
        "> GPT-2 Configurations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZD29VGy11LXz"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024\n",
        "    vocab_size: int = 50304\n",
        "    n_layer: int = 12\n",
        "    n_head: int = 12\n",
        "    n_embd: int = 768\n",
        "    drop_rate: float = 0.0\n",
        "    bias: bool = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K2eE7SIdCK_u",
        "outputId": "c1a6de47-ca26-4215-be27-683be41cf0fc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "number of parameters: 123.59M\n"
          ]
        }
      ],
      "source": [
        "checkpts_dir = 'path to your checkpoints'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "ckpt_path = os.path.join(checkpts_dir, 'ckpt.pt')\n",
        "checkpoint = torch.load(ckpt_path, map_location=device)\n",
        "gptconf = GPTConfig(**checkpoint['model_args'])\n",
        "model = GPT(gptconf)\n",
        "state_dict = checkpoint['model']\n",
        "unwanted_prefix = '_orig_mod.'\n",
        "for k,v in list(state_dict.items()):\n",
        "    if k.startswith(unwanted_prefix):\n",
        "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
        "model.load_state_dict(state_dict)\n",
        "\n",
        "# compile model\n",
        "model.eval()\n",
        "model.to(device)\n",
        "if compile:\n",
        "    model = torch.compile(model)\n",
        "\n",
        "# tokenizer\n",
        "tokenizer = tiktoken.get_encoding(\"r50k_base\") \n",
        "encode = lambda s: tokenizer.encode(s, allowed_special={\"<|endoftext|>\"})\n",
        "decode = lambda l: tokenizer.decode(l)\n",
        "\n",
        "\n",
        "# inference\n",
        "def generate(prompt, num_samples=1, max_tokens=50, temp=1, top_k=100):\n",
        "    start_ids = encode(prompt)\n",
        "    x = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n",
        "\n",
        "    # generate\n",
        "    for k in range(num_samples):\n",
        "        print(f'sample num:[{k}]')\n",
        "        y = model.generate(x, max_tokens, temperature=temp, top_k=top_k)\n",
        "        print(decode(y[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5cIosFssm7X",
        "outputId": "5a8d0d6c-cbe1-4313-c53f-ac1fc5157693"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sample num:[0]\n",
            "3D Smart factory is a wonderful company because it’s so well made and doesn io9070101 as an easy to use device. By using e070111 a smart camera can be used on the smart phone and a smart phone. Smartphones come from devices which can be used on Android phones, tablets, screens, etc. I could end up with smart phones and can easily run smart home devices on smartphones. Smartphones come from devices which don’t hold any information about the devices themselves but can be used for\n",
            "sample num:[1]\n",
            "3D Smart factory is a wonderful company because we're not the first one in manufacturing. I know my wife and I use these things to make products, but I can't, because they're just not the products that we use. So I use these designs on my wedding ring, because I love and I love them. So I'm not going to give up on any for sale! Anyways, if you have a brand name you're going to have a name called \"Formula\" in your name, which is you're going to\n",
            "sample num:[2]\n",
            "3D Smart factory is a wonderful company because it always has a proven track record of delivering on the world. But it is also a company that makes for just about any product. With SmartThings, a smart smart device is the only device in the world, whereas the blockchain is an app store. We are all very proud of SmartThings, by our side, and thank you so much to all Apple fans who love SmartThings. The SmartThings system is designed with a simple design. The SmartThings SmartApps are small gadgets that can be\n",
            "sample num:[3]\n",
            "3D Smart factory is a wonderful company because it’s easy to use, easy to control and reliable in a number of different environments. Smart Smart Smart Home Homeacciode Smart Homeacciode is basically a Homeacciode Smart Homeacciode and is one of the most versatile Smart Homeacciode sensors in the world, because it can handle different types of devices, and allows glucose sensors to transfer into the Smart Homeacciode Smart Homeacciode Smart Homeacciode smart Homeacciode sensors are all able to provide data analytics, so\n",
            "sample num:[4]\n",
            "3D Smart factory is a wonderful company because it is the perfect combination to solve the problems discussed above, making such a highly efficient solution for space colonization. The technology is easy to scale, and many people who go to other cities using technology such as space colonization are doing it the most important part of the day. It is not just in space colonization but actually in space colonization. The technology can be built, but it can also be built. There is no other technology that can be built at the time, and so space colonization is not feasible\n",
            "sample num:[5]\n",
            "3D Smart factory is a wonderful company because of its simplicity and efficiency. We have built smart prototypes for Smart and connected home devices, so while BlackBerry has always been considered an essential industry for the team, it does have a consistent product line. The software is fairly straightforward. It’s easy to pick up a device we do not have a smartphone at the moment, but it is a necessity. The smartphone is fast, responsive, and responsive. It’s easy to pick up a phone. Let’s take a look\n",
            "sample num:[6]\n",
            "3D Smart factory is a wonderful company because it's not a great solution for any engine. It's a product of value that is valued over time by many vendors. You have no real value to the product. You have a way to sell products. The price of a product is not worth it. If the price is worth it, the cost of the product will be immense. Why will it never be a good product? The answer is to buy a product that is worth more than the cost of the product. This is a question that\n",
            "sample num:[7]\n",
            "3D Smart factory is a wonderful company because it makes a lot of money. It’s a lot more than just a one-two-two-two-two-two-two-two-two-two-three-two-two-two-two-two-two-two-two variant with a single Smart factory. If we had a Smart factory, the people were going to be able to get the Smart factory in Canada.” The result has been a much higher number of calls made. “We\n",
            "sample num:[8]\n",
            "3D Smart factory is a wonderful company because it can always be opened inside it’s own design ecosystem. I know there are a lot of different kinds of products in the market that are still open, but I can watch SmartOne videos, for example, as well as my personal SmartOne. SmartOne is also a beautiful product that I actually use. SmartOne is one that has the most unique smart One category, but it also has a lot of design elements. SmartOne is not a smart One that I think the SmartOne\n",
            "sample num:[9]\n",
            "3D Smart factory is a wonderful company because it offers everything you need in a Raspberry Pi, Raspberry Pi and other DIY DIY gadgets. This is all well and easy to install. If you are struggling from a Raspberry Pi board you likely won’t find a place to deposit a Raspberry Pi Pi Pi (or other DIY computer type, for that matter) and then download the Raspberry Pi PiPi, which is an easy install to move from a Raspberry Pi board with the Raspberry Pi software, and then download it. The Raspberry PiPi is\n"
          ]
        }
      ],
      "source": [
        "num_samples = 10\n",
        "max_tokens = 100\n",
        "temp = 0.80\n",
        "top_k = 100\n",
        "\n",
        "prompt = \"3D Smart factory is a wonderful company because\"\n",
        "\n",
        "generate(prompt, num_samples, max_tokens, temp, top_k)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "KWsFqyZ9eX-p",
        "OlqdCZome_NA"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
